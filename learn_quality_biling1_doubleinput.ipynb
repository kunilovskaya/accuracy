{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script is an attempt to classify good and bad translations from RusLTC on bilingual vectorised representations of lempos (no stopwords) in sources and targets\n",
    "\n",
    "supposedly in this set up the algorithm captures accuracy of the translation rather than (morphosyntactic) fluency\n",
    "\n",
    "Ideas to try out after the failure of the initial naive approach that also lacked in theoretical justification (concatenated ST and TT, represented with respective bilingual embeddings and padded to the max text length treated as instances)\n",
    "\n",
    "1. go for decontextualised sentences, not texts: get sentences with annotated content errors from bad translations and error-free sentences from good translations for classes\n",
    "2. represent texts as a sequence of vectorised sentences: assign extra weights on badly translated sentences in bad translations (based on error stats normalised per word)\n",
    "3. two inputs to the LSTM: pass aligned sources and targets separately, measure similarity between the outputs of the two biLSTMs and use these similarities as input to the Dense Output layer (542 similarities are weighted to get 542 prediction outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "## this is supposed to ensure reproducible results at each run of the script\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import io ### https://docs.python.org/3/library/io.html\n",
    "import sys,os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "import gensim\n",
    "from collections import OrderedDict\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras import backend, preprocessing\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Lambda, concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import load_model as load_keras_model\n",
    "from keras.layers import Embedding\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "## to avoid reloading kernel after changes to imported modules\n",
    "# import importlib\n",
    "# import HTQ_functions as mm\n",
    "# importlib.reload(mm)\n",
    "\n",
    "## import the functions from the helper scripts\n",
    "# from acc_functions import \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparedata(directory):\n",
    "    ourdic = []\n",
    "    print('Collecting data from the files...')\n",
    "    for subdir in os.listdir(directory):\n",
    "        files = [f for f in os.listdir(os.path.join(directory, subdir)) if f.endswith('.conllu')]\n",
    "        for f in files:\n",
    "            rowdic = {'doc': f.strip(), 'group': subdir}\n",
    "            doc = open(os.path.join(directory, subdir, f))\n",
    "            text = doc.read().strip() #.replace('\\n', ' ')\n",
    "            doc.close()\n",
    "            rowdic['text'] = text\n",
    "            ourdic.append(rowdic)\n",
    "    ourdic = pd.DataFrame(ourdic)\n",
    "    return ourdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data from the files...\n"
     ]
    }
   ],
   "source": [
    "## preprosessing from https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_udpipe.py\n",
    "datadir = '/home/masha/accuracy/data/lempos/'  # The path to where subdirectories whith text files are\n",
    "df = preparedata(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=====transform the df to align each target with its source: doc, src, tgt, label=====**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>group</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "      <td>source</td>\n",
       "      <td>pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>EN_1_102.conllu</td>\n",
       "      <td>source</td>\n",
       "      <td>anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>EN_1_114.conllu</td>\n",
       "      <td>source</td>\n",
       "      <td>product_NOUN placement_NOUN on_ADV broadway_VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>EN_1_116.conllu</td>\n",
       "      <td>source</td>\n",
       "      <td>d_PROPN 'oh_PROPN german_PROPN broadcaster_PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>EN_1_121.conllu</td>\n",
       "      <td>source</td>\n",
       "      <td>how_ADV be_AUX you_PRON social_PROPN security_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 doc   group  \\\n",
       "608  EN_1_101.conllu  source   \n",
       "580  EN_1_102.conllu  source   \n",
       "576  EN_1_114.conllu  source   \n",
       "557  EN_1_116.conllu  source   \n",
       "615  EN_1_121.conllu  source   \n",
       "\n",
       "                                                  text  \n",
       "608  pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...  \n",
       "580  anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...  \n",
       "576  product_NOUN placement_NOUN on_ADV broadway_VE...  \n",
       "557  d_PROPN 'oh_PROPN german_PROPN broadcaster_PRO...  \n",
       "615  how_ADV be_AUX you_PRON social_PROPN security_...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_df = df[~(df['group']=='source')]\n",
    "print(targets_df.shape)\n",
    "sources_df = df[df['group']=='source']\n",
    "sources_df = sources_df.sort_values('doc')\n",
    "sources_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>group</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU_1_244_35.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>весь_DET правда_NOUN о_ADP гмый_PROPN сторонни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU_1_271_4.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>не_PART посещать_VERB 'д_NOUN странный_ADJ реб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU_1_150_2.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>последний_ADJ книга_NOUN роджер::пенроуз_PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU_1_244_1.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>весь_DET правда_NOUN о_ADP генетически_ADV мод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU_1_274_41.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>газета_PROPN нью-йорк::таймз_PROPN xx_NUM октя...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc group source  \\\n",
       "0  RU_1_244_35.conllu   bad   None   \n",
       "1   RU_1_271_4.conllu   bad   None   \n",
       "2   RU_1_150_2.conllu   bad   None   \n",
       "3   RU_1_244_1.conllu   bad   None   \n",
       "4  RU_1_274_41.conllu   bad   None   \n",
       "\n",
       "                                              target  \n",
       "0  весь_DET правда_NOUN о_ADP гмый_PROPN сторонни...  \n",
       "1  не_PART посещать_VERB 'д_NOUN странный_ADJ реб...  \n",
       "2  последний_ADJ книга_NOUN роджер::пенроуз_PROPN...  \n",
       "3  весь_DET правда_NOUN о_ADP генетически_ADV мод...  \n",
       "4  газета_PROPN нью-йорк::таймз_PROPN xx_NUM октя...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned = targets_df.copy()\n",
    "aligned.insert(loc=2, column='source', value=None)\n",
    "aligned.columns = ['doc', 'group', 'source', 'target']\n",
    "\n",
    "aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>group</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>RU_1_101_9.conllu</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "      <td>pussy_PROPN riot_PROPN путин_PROPN против_ADP ...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>RU_1_101_3.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>пусси::райот_PROPN путин_PROPN против_ADP панк...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RU_1_101_8.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>пусси::райот_PROPN судебный_ADJ разбирательств...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>RU_1_102_6.conllu</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "      <td>служба_NOUN по_ADP борьба_NOUN с_ADP наркотик_...</td>\n",
       "      <td>EN_1_102.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>RU_1_102_15.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>None</td>\n",
       "      <td>управление_PROPN по_ADP борьба_NOUN с_ADP нарк...</td>\n",
       "      <td>EN_1_102.conllu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc group source  \\\n",
       "254   RU_1_101_9.conllu  good   None   \n",
       "62    RU_1_101_3.conllu   bad   None   \n",
       "11    RU_1_101_8.conllu   bad   None   \n",
       "471   RU_1_102_6.conllu  good   None   \n",
       "119  RU_1_102_15.conllu   bad   None   \n",
       "\n",
       "                                                target             temp  \n",
       "254  pussy_PROPN riot_PROPN путин_PROPN против_ADP ...  EN_1_101.conllu  \n",
       "62   пусси::райот_PROPN путин_PROPN против_ADP панк...  EN_1_101.conllu  \n",
       "11   пусси::райот_PROPN судебный_ADJ разбирательств...  EN_1_101.conllu  \n",
       "471  служба_NOUN по_ADP борьба_NOUN с_ADP наркотик_...  EN_1_102.conllu  \n",
       "119  управление_PROPN по_ADP борьба_NOUN с_ADP нарк...  EN_1_102.conllu  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned['temp'] = aligned['doc'].replace(to_replace='RU',value=r'EN', regex=True)\n",
    "aligned['temp'] = aligned['temp'].replace(to_replace='_\\d+\\.conllu',value=r'.conllu', regex=True)\n",
    "aligned = aligned.sort_values('temp')\n",
    "aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>group</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>RU_1_101_9.conllu</td>\n",
       "      <td>good</td>\n",
       "      <td>pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...</td>\n",
       "      <td>pussy_PROPN riot_PROPN путин_PROPN против_ADP ...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>RU_1_101_3.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...</td>\n",
       "      <td>пусси::райот_PROPN путин_PROPN против_ADP панк...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RU_1_101_8.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...</td>\n",
       "      <td>пусси::райот_PROPN судебный_ADJ разбирательств...</td>\n",
       "      <td>EN_1_101.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>RU_1_102_6.conllu</td>\n",
       "      <td>good</td>\n",
       "      <td>anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...</td>\n",
       "      <td>служба_NOUN по_ADP борьба_NOUN с_ADP наркотик_...</td>\n",
       "      <td>EN_1_102.conllu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>RU_1_102_15.conllu</td>\n",
       "      <td>bad</td>\n",
       "      <td>anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...</td>\n",
       "      <td>управление_PROPN по_ADP борьба_NOUN с_ADP нарк...</td>\n",
       "      <td>EN_1_102.conllu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc group  \\\n",
       "254   RU_1_101_9.conllu  good   \n",
       "62    RU_1_101_3.conllu   bad   \n",
       "11    RU_1_101_8.conllu   bad   \n",
       "471   RU_1_102_6.conllu  good   \n",
       "119  RU_1_102_15.conllu   bad   \n",
       "\n",
       "                                                source  \\\n",
       "254  pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...   \n",
       "62   pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...   \n",
       "11   pussy_PROPN riot_PROPN putin_ADJ v_NOUN punk_N...   \n",
       "471  anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...   \n",
       "119  anti_PROPN drug_PROPN agency_PROPN seeks_PROPN...   \n",
       "\n",
       "                                                target             temp  \n",
       "254  pussy_PROPN riot_PROPN путин_PROPN против_ADP ...  EN_1_101.conllu  \n",
       "62   пусси::райот_PROPN путин_PROPN против_ADP панк...  EN_1_101.conllu  \n",
       "11   пусси::райот_PROPN судебный_ADJ разбирательств...  EN_1_101.conllu  \n",
       "471  служба_NOUN по_ADP борьба_NOUN с_ADP наркотик_...  EN_1_102.conllu  \n",
       "119  управление_PROPN по_ADP борьба_NOUN с_ADP нарк...  EN_1_102.conllu  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfns = aligned['temp'].tolist()\n",
    "for i in sfns:\n",
    "## two ways of putting the value from one column (text) in one df on the value in the other column (temp filename) into the specified column (source) of the other df, following the list respecting the order of the cells \n",
    "#     aligned.loc[aligned.temp == i, 'source'] = sources_df[sources_df['doc']=='EN_1_101.conllu']['text'].values[0]\n",
    "    aligned.loc[aligned.temp == i, 'source'] = sources_df.loc[sources_df['doc'] == i, 'text'].item()\n",
    "aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-11 01:00:31,948 : INFO : Number of classes: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Distribution of classes in the dataset:\n",
      "       doc  source  target  temp\n",
      "group                           \n",
      "bad    213     213     213   213\n",
      "good   329     329     329   329\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "ru_xtrain0 = aligned['target']\n",
    "en_xtrain0 = aligned['source']\n",
    "\n",
    "yy_train = aligned['group'].tolist()\n",
    "\n",
    "classes = sorted(list(set(aligned['group'].tolist())))\n",
    "num_classes = len(classes)\n",
    "logger.info('Number of classes: %d' % num_classes)\n",
    "print('===========================')\n",
    "print('Distribution of classes in the dataset:')\n",
    "print(aligned.groupby('group').count())\n",
    "print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random target doc: ['ученый_PROPN', 'против_ADP', 'нобелевский_ADJ', 'премия_NOUN', 'нобелевский_ADJ', 'премия_NOUN', 'по_ADP', 'психология_NOUN', 'медицина_NOUN', 'физик_NOUN', 'или_CCONJ', 'химия_NOUN', 'это_PRON', 'самый_ADJ', 'авторитетный_ADJ', 'и_CCONJ', 'почетный_ADJ', 'награда_NOUN', 'в_ADP', 'область_NOUN']\n",
      "\n",
      "First target doc: ['весь_DET', 'правда_NOUN', 'о_ADP', 'гмый_PROPN', 'сторонник_NOUN', 'генномодифицировать_ADJ', 'зерно_NOUN', 'настаивать_VERB', 'что_SCONJ', 'это_PRON', 'единственный_ADJ', 'способ_NOUN', 'справляться_VERB', 'с_ADP', 'то_PRON', 'что_SCONJ', 'прокармливать_VERB', 'густонаселенный_ADJ', 'мир_NOUN', 'критика_NOUN']\n"
     ]
    }
   ],
   "source": [
    "## Have a look at a random document \n",
    "doc = random.choice(range(20))\n",
    "# print(type(ru_xtrain0[doc]))\n",
    "print('Random target doc:', ru_xtrain0[doc].split()[:20])\n",
    "## or at the first text to avoid an extra variable\n",
    "print('\\nFirst target doc:', ru_xtrain0[0].split()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=====a neural model with an biLSTM layer=======** \n",
    "\n",
    "that is going to look at every word in ST+TT represented by corresponding bilingual embeddings of size 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-11 01:00:32,081 : INFO : loading projection weights from /home/masha/MUSE/rig1_res/vectors-en.vec\n",
      "2019-10-11 01:01:27,606 : INFO : loaded (296630, 300) matrix from /home/masha/MUSE/rig1_res/vectors-en.vec\n",
      "2019-10-11 01:01:27,607 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-10-11 01:01:29,588 : INFO : loading projection weights from /home/masha/MUSE/rig1_res/vectors-ru.vec\n",
      "2019-10-11 01:02:17,243 : INFO : loaded (248978, 300) matrix from /home/masha/MUSE/rig1_res/vectors-ru.vec\n",
      "2019-10-11 01:02:17,244 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-10-11 01:02:18,960 : INFO : Loading word vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "296630\n",
      "['Hou_PROPN', 'Aeronautica_PROPN', 'imagination_NOUN', 'Vélez::Sársfield_PROPN', 'shotcrete_NOUN']\n",
      "['андрей::голубев_PROPN', 'заслуживать_VERB', 'sag_PROPN', 'женькин_ADJ', 'huey_PROPN']\n"
     ]
    }
   ],
   "source": [
    "### concatenate embeddings externally and use the new embeddings file in this function (mind the necessity to update the first line and delete the first line of the appended file)\n",
    "src_path = '/home/masha/MUSE/rig1_res/vectors-en.vec'\n",
    "tgt_path = '/home/masha/MUSE/rig1_res/vectors-ru.vec'\n",
    "def load_embeddings(embeddings_file):\n",
    "    # Определяем формат модели по её расширению:\n",
    "    if embeddings_file.endswith('.bin.gz') or embeddings_file.endswith('.bin'):  # Бинарный формат word2vec\n",
    "        emb_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            embeddings_file, binary=True, unicode_errors='replace')\n",
    "    elif embeddings_file.endswith('.txt.gz') or embeddings_file.endswith('.txt') \\\n",
    "            or embeddings_file.endswith('.vec.gz') or embeddings_file.endswith('.vec'):  # Текстовый формат word2vec\n",
    "        emb_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            embeddings_file, binary=False, unicode_errors='ignore') ##unicode_errors='replace'\n",
    "    else:  # Нативный формат Gensim?\n",
    "        emb_model = gensim.models.KeyedVectors.load(embeddings_file)\n",
    "    emb_model.init_sims(replace=True)  # normalise vectors just in case\n",
    "    return emb_model\n",
    "\n",
    "en_embeddings = load_embeddings(src_path)\n",
    "ru_embeddings = load_embeddings(tgt_path)\n",
    "\n",
    "logger.info('Loading word vectors')\n",
    "\n",
    "## an alternative (and leaner) way to build model vocabulary\n",
    "# ext_vocab = external_embeddings.index2entity\n",
    "# ext_word_index = {}\n",
    "#     for nr, word in enumerate(ext_vocab):\n",
    "#         ext_word_index[word] = nr\n",
    "\n",
    "en_voc = en_embeddings.vocab\n",
    "ru_voc = ru_embeddings.vocab\n",
    "print(type(en_voc))\n",
    "print(len(en_voc))\n",
    "print(list(en_voc.keys())[0:5])\n",
    "print(list(ru_voc.keys())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert class labels into indices\n",
    "y_train0 = [classes.index(i) for i in yy_train]\n",
    "print(y_train0[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a binary classification (see number of columns): (542, 2)\n"
     ]
    }
   ],
   "source": [
    "## Convert indices to categorical values to use with (binary_ or categorical_)crossentropy loss\n",
    "y_train = to_categorical(y_train0, num_classes)\n",
    "print('We have a binary classification (see number of columns):', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word, vocabulory=None):\n",
    "    ## get word index in the models vocabulary\n",
    "    if word in vocabulory:\n",
    "        return vocabulory[word].index\n",
    "    ## set index to OOV items like putin_ADJ to 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First source text: ['the_DET', 'truth_NOUN', 'about_ADP', 'genetically_ADV', 'modify_ADJ', 'food_NOUN', 'proponent_NOUN', 'of_ADP', 'genetically_ADV', 'modify_VERB', 'crops_NOUN', 'say_VERB', 'the_DET', 'technology_NOUN', 'be_AUX', 'the_DET', 'only_ADJ', 'way_NOUN', 'to_PART', 'feed_VERB']\n",
      "\n",
      "The first of the aligned translations: ['весь_DET', 'правда_NOUN', 'о_ADP', 'гмый_PROPN', 'сторонник_NOUN', 'генномодифицировать_ADJ', 'зерно_NOUN', 'настаивать_VERB', 'что_SCONJ', 'это_PRON', 'единственный_ADJ', 'способ_NOUN', 'справляться_VERB', 'с_ADP', 'то_PRON', 'что_SCONJ', 'прокармливать_VERB', 'густонаселенный_ADJ', 'мир_NOUN', 'критика_NOUN']\n"
     ]
    }
   ],
   "source": [
    "print('First source text:', aligned['source'][0].split()[:20])\n",
    "print('\\nThe first of the aligned translations:', aligned['target'][0].split()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First source text: [0, 0, 0, 0, 3814, 0, 87, 0, 948, 0, 0, 0, 0, 163, 0, 5402, 9, 0, 807, 69]\n",
      "First source text with no OOV (tot num of enOOV=123146! ruOOV=71558): [3814, 87, 948, 163, 5402, 9, 807, 69, 1215, 1545, 12455, 96613, 82718, 153, 90, 278, 1455, 4675, 402, 31]\n"
     ]
    }
   ],
   "source": [
    "##convert them to embedding indices in a joint biling embedding file\n",
    "entrain0 = [[get_index(w,en_voc) for w in text.split()] for text in aligned['source']]\n",
    "rutrain0 = [[get_index(w,ru_voc) for w in text.split()] for text in aligned['target']]\n",
    "\n",
    "## here we are getting rid of OOV items:\n",
    "entrain = [[id for id in text if id != 0] for text in entrain0]\n",
    "rutrain = [[id for id in text if id != 0] for text in rutrain0]\n",
    "\n",
    "print('First source text:', entrain0[0][:20])\n",
    "print('First source text with no OOV (tot num of enOOV=123146! ruOOV=71558):', entrain[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-11 01:02:19,230 : INFO : Average SOURCE text length: 198.5 tokens\n",
      "2019-10-11 01:02:19,232 : INFO : Average TARGET text length: 257.5 tokens\n",
      "2019-10-11 01:02:19,233 : INFO : Maximum SOURCE text length: 403.0 tokens\n",
      "2019-10-11 01:02:19,234 : INFO : Maximum TARGET text length: 685.0 tokens\n"
     ]
    }
   ],
   "source": [
    "logger.info('Average SOURCE text length: %s tokens'\n",
    "                % \"{0:.1f}\".format(np.mean(list(map(len, entrain))), 1)) #aligned['source'].str.split()\n",
    "logger.info('Average TARGET text length: %s tokens'\n",
    "                % \"{0:.1f}\".format(np.mean(list(map(len, rutrain))), 1))\n",
    "logger.info('Maximum SOURCE text length: %s tokens'\n",
    "                % \"{0:.1f}\".format(np.max(list(map(len, entrain))), 1))\n",
    "logger.info('Maximum TARGET text length: %s tokens'\n",
    "                % \"{0:.1f}\".format(np.max(list(map(len, rutrain))), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation turned into [38362 26358  5961 18779  1532] ...,\n",
      " its length is 685\n",
      "Data parameters: (542, 685)\n"
     ]
    }
   ],
   "source": [
    "# Padding: make all texts lengths equal to absolute max in the data by filling in zeros\n",
    "max_seq_length = 685  \n",
    "vectorized_en = preprocessing.sequence.pad_sequences(\n",
    "    entrain, maxlen=max_seq_length, truncating='post', padding='post')\n",
    "vectorized_ru = preprocessing.sequence.pad_sequences(\n",
    "    rutrain, maxlen=max_seq_length, truncating='post', padding='post')\n",
    "print('The translation turned into %s ...,\\n its length is %s' % (vectorized_ru[0][:5], len(vectorized_ru[0]))) #, \n",
    "print('Data parameters:', vectorized_ru.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[3814   87  948 ...    0    0    0]\n",
      " [3814   87  948 ...    0    0    0]\n",
      " [3814   87  948 ...    0    0    0]\n",
      " [ 885 1089  698 ...    0    0    0]\n",
      " [ 885 1089  698 ...    0    0    0]]\n",
      "542\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_en))\n",
    "print(vectorized_en[:5])\n",
    "print(len(vectorized_en))\n",
    "print(len(vectorized_ru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====BUILDING the model====**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see https://github.com/keras-team/keras/issues/5541\n",
    "## https://gist.github.com/GKarmakar/3aa0c643ddb0688a9bfc44b43b84edd8\n",
    "\n",
    "## K.exp = math_ops.exp(x)???\n",
    "## https://www.tensorflow.org/api_docs/python/tf/math/exp: Computes exponential of x element-wise: y= e^x\n",
    "## about e=2.7182818 (Euler's number) https://www.mathsisfun.com/numbers/e-eulers-number.html\n",
    "\n",
    "import keras.backend as K\n",
    "## a negative exponent can be expressed as its positive reciprocal: 5^-3 = 1/5^3, so the result of my function is either pos or neg, but always smaller than -1:1\n",
    "def exponent_neg_manhattan_distance(source, target):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(source - target), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def exponent_neg_euclidean_distance(source, target):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    ## K.exp() is a constant e to the power of -x, where x is the distance; the purpose of it is to put the result into the range of -1;1\n",
    "    return K.exp(-K.sqrt(K.sum(K.square(source - target), axis=1, keepdims=True)))\n",
    "\n",
    "def exponent_neg_cosine_distance(source, target):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "\n",
    "    return K.exp(K.dot(source, target) / (K.sqrt(K.dot(source, source)) * K.sqrt(K.dot(target,target))), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(loss='binary_crossentropy'): # 'binary_crossentropy', 'categorical_crossentropy'\n",
    "    \n",
    "    ## encoder\n",
    "    en_input = Input(shape=(max_seq_length,), name='ST_wsequences')\n",
    "    ## decoder\n",
    "    ru_input = Input(shape=(max_seq_length,), name='TT_wsequences')\n",
    "    \n",
    "    ## represent words in texts with their embeddings (what happens to OOVwords with index=0)\n",
    "    en_vectors = en_embeddings.vectors\n",
    "    ru_vectors = ru_embeddings.vectors\n",
    "    print(ru_vectors.shape)\n",
    "    ## Input shape: 2D tensor with shape: (batch_size, sequence_length)\n",
    "    ## Output shape: 3D tensor with shape: (batch_size, sequence_length, output_dim)\n",
    "    en_emb = Embedding(input_dim=en_vectors.shape[0], output_dim=en_vectors.shape[1], weights=[en_vectors],\n",
    "                    input_length=max_seq_length, trainable=False, name='en_embeddings')(en_input)\n",
    "    ru_emb = Embedding(input_dim=ru_vectors.shape[0], output_dim=ru_vectors.shape[1], weights=[ru_vectors],\n",
    "                input_length=max_seq_length, trainable=False, name='ru_embeddings')(ru_input)\n",
    "    # units is a hyperparameter; the output of the lstm layer\n",
    "    # this is where the model would process the input sequence in both directions\n",
    "    \n",
    "    # To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want\n",
    "    ## # When we reuse the same layer instance, the weights of the layer are also being reused\n",
    "    ## (it is effectively *the same* layer)\n",
    "    shared_lstm = LSTM(units=128, name=\"biLSTM\") ##Bidirectional(\n",
    "    \n",
    "    en_x = shared_lstm(en_emb)  \n",
    "    ru_x = shared_lstm(ru_emb)\n",
    "    \n",
    "    # Calculates the pair-wise distance between ST and TT; the output should be length 542\n",
    "#     distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "#                                output_shape=lambda x: (x[0][0], 1))([en_x, ru_x])\n",
    "#     distance = Lambda(function=lambda x: exponent_neg_euclidean_distance(x[0], x[1]),\n",
    "#                                output_shape=lambda x: (x[0][0], 1))([en_x, ru_x])\n",
    "    \n",
    "#     print(type(distance))\n",
    "#     print(distance.shape)\n",
    "    \n",
    "    ## instead of similarity, use concatenation of the text vectors output by LSTMs:\n",
    "    merged = concatenate([en_x, ru_x], axis=-1)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax', name='Output')(merged) # distance # this is the output layer which predicts the labels\n",
    "\n",
    "    ## compile the model\n",
    "    model = Model(inputs=[en_input, ru_input], outputs=output)\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # print the model architecture\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248978, 300)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ST_wsequences (InputLayer)      (None, 685)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TT_wsequences (InputLayer)      (None, 685)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "en_embeddings (Embedding)       (None, 685, 300)     88989000    ST_wsequences[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ru_embeddings (Embedding)       (None, 685, 300)     74693400    TT_wsequences[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "biLSTM (LSTM)                   (None, 128)          219648      en_embeddings[0][0]              \n",
      "                                                                 ru_embeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           biLSTM[0][0]                     \n",
      "                                                                 biLSTM[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 2)            514         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 163,902,562\n",
      "Trainable params: 220,162\n",
      "Non-trainable params: 163,682,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model0 = createmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights calculated from the distribution: {0: 1.272300469483568, 1: 0.8237082066869301}\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Train on 487 samples, validate on 55 samples\n",
      "Epoch 1/10\n",
      "487/487 [==============================] - 58s 120ms/step - loss: 0.6976 - acc: 0.6099 - val_loss: 0.6895 - val_acc: 0.6000\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 0.6936 - acc: 0.5298 - val_loss: 0.6926 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "104/487 [=====>........................] - ETA: 27s - loss: 0.6746 - acc: 0.6635"
     ]
    }
   ],
   "source": [
    "# Train the model on our data\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## compute the class weights to pass to the scoring function which generates the update weights (values)\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train0),\n",
    "                                                 y_train0)\n",
    "## convert class_weights to dict\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print('class weights calculated from the distribution:',class_weight_dict)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, verbose=1, mode='max')\n",
    "# val_split = 0.1 \n",
    "\n",
    "start = time.time()\n",
    "## batch_size: number of samples per gradient update.\n",
    "## verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "## validation_split: evaluate the loss and any model metrics on this data at the end of each epoch\n",
    "## shuffle: whether to shuffle the training data before each epoch\n",
    "## class_weight: \"pay more attention\" to samples from an under-represented class; https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "## {0: 2., 1: 1.} \"treat every instance of class 0 (bad) as 2 instances of class 0\" > the loss becomes a weighted average\n",
    "\n",
    "## make sure that the testset held out for testing each epoch and evaluating the model has a proportional distribution of each class samples\n",
    "enX_train, enX_test, Y_train, Y_test = train_test_split(vectorized_en, y_train,\n",
    "                                                    stratify=y_train, \n",
    "                                                    test_size=0.1, random_state=42)\n",
    "print(Y_test[:5])\n",
    "ruX_train, ruX_test, Y_train0, Y_test0 = train_test_split(vectorized_ru, y_train,\n",
    "                                                    stratify=y_train, \n",
    "                                                    test_size=0.1, random_state=42)\n",
    "# print('test that Ys are the same': Y_test, Y_test)\n",
    "## iterating on the data in batches of 4 samples\n",
    "history = model0.fit([enX_train,ruX_train], Y_train, epochs=10, verbose=1, validation_data=([enX_test,ruX_test],Y_test),\n",
    "         batch_size=4, shuffle=False, class_weight=class_weight_dict,callbacks=[earlystopping]) ## class_weight=class_weight_dict, class_weight=class_weight_dict, validation_split=val_split\n",
    "\n",
    "end = time.time()\n",
    "training_time = int(end - start)\n",
    "logger.info('Trained in %s minutes' % str(round(training_time/60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "55/55 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-11 01:27:12,387 : INFO : Loss function value: 0.6730\n",
      "2019-10-11 01:27:12,388 : INFO : Accuracy on the test set: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model:\n",
    "print(model0.metrics_names)\n",
    "score = model0.evaluate([enX_test,ruX_test],Y_test, verbose=1)\n",
    "logger.info('Loss function value: %s' % \"{0:.4f}\".format(score[0]))\n",
    "logger.info('Accuracy on the test set: %s' % \"{0:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию из sklearn чтобы посчитать F1 по каждому классу:\n",
    "predictions = model0.predict([enX_test,ruX_test])\n",
    "predictions = np.around(predictions)  # проецируем предсказания модели в бинарный диапазон {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-11 01:27:14,665 : INFO : Classification results on the test set:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2019-10-11 01:27:14,676 : INFO : Macro-F1 on the test set: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.00      0.00      0.00        22\n",
      "        good       0.60      1.00      0.75        33\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        55\n",
      "   macro avg       0.30      0.50      0.37        55\n",
      "weighted avg       0.36      0.60      0.45        55\n",
      "\n",
      "Confusion matrix\n",
      " [[ 0 22]\n",
      " [ 0 33]]\n"
     ]
    }
   ],
   "source": [
    "# Конвертируем предсказания обратно из чисел в текстовые метки классов\n",
    "y_test_real = [classes[np.argmax(pred)] for pred in Y_test]\n",
    "predictions = [classes[np.argmax(pred)] for pred in predictions]\n",
    "\n",
    "logger.info('Classification results on the test set:')\n",
    "print(classification_report(y_test_real, predictions))\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(y_test_real, predictions))\n",
    "\n",
    "fscore = precision_recall_fscore_support(y_test_real, predictions, average='macro')[2]\n",
    "logger.info('Macro-F1 on the test set: %s' % \"{0:.4f}\".format(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_CPUkerasTF",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
